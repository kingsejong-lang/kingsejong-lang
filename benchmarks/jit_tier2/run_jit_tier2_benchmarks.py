#!/usr/bin/env python3
"""
JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ì„±ëŠ¥ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python3 run_jit_tier2_benchmarks.py [--interpreter PATH] [--runs N]
"""

import subprocess
import time
import sys
import os
import argparse
import re
from pathlib import Path

class JITTier2BenchmarkRunner:
    def __init__(self, interpreter_path, num_runs=5):
        self.interpreter = interpreter_path
        self.num_runs = num_runs
        self.results = {}

    def run_benchmark(self, benchmark_file):
        """ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° JIT í†µê³„ ìˆ˜ì§‘"""
        times = []
        jit_stats = None

        for i in range(self.num_runs):
            start = time.time()
            try:
                # --jit-stats í”Œë˜ê·¸ë¡œ JIT í†µê³„ ì¶œë ¥
                result = subprocess.run(
                    [self.interpreter, "--jit-stats", benchmark_file],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                end = time.time()

                if result.returncode != 0:
                    print(f"  âŒ ì‹¤í–‰ ì‹¤íŒ¨: {benchmark_file}")
                    print(f"     ì˜¤ë¥˜: {result.stderr}")
                    return None, None

                elapsed = end - start
                times.append(elapsed)
                print(f"  Run {i+1}/{self.num_runs}: {elapsed:.4f}s")

                # ì²« ë²ˆì§¸ ì‹¤í–‰ì—ì„œ JIT í†µê³„ ìˆ˜ì§‘
                if i == 0:
                    jit_stats = self.parse_jit_stats(result.stderr)

            except subprocess.TimeoutExpired:
                print(f"  â±ï¸ íƒ€ì„ì•„ì›ƒ: {benchmark_file}")
                return None, None
            except Exception as e:
                print(f"  âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
                return None, None

        return times, jit_stats

    def parse_jit_stats(self, output):
        """JIT í†µê³„ íŒŒì‹±"""
        stats = {
            'tier1_compilations': 0,
            'tier2_compilations': 0,
            'tier1_executions': 0,
            'tier2_executions': 0,
            'total_inlined_functions': 0
        }

        # Tier 1 í†µê³„
        match = re.search(r'Total Compilations:\s+(\d+)', output)
        if match:
            stats['tier1_compilations'] = int(match.group(1))

        match = re.search(r'Total Executions:\s+(\d+)', output)
        if match:
            stats['tier1_executions'] = int(match.group(1))

        # Tier 2 í†µê³„
        match = re.search(r'Tier 2 Compilations:\s+(\d+)', output)
        if match:
            stats['tier2_compilations'] = int(match.group(1))

        match = re.search(r'Tier 2 Executions:\s+(\d+)', output)
        if match:
            stats['tier2_executions'] = int(match.group(1))

        match = re.search(r'Total Inlined Functions:\s+(\d+)', output)
        if match:
            stats['total_inlined_functions'] = int(match.group(1))

        return stats

    def calculate_stats(self, times):
        """í†µê³„ ê³„ì‚°"""
        if not times:
            return None

        avg = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        # í‘œì¤€í¸ì°¨ ê³„ì‚°
        variance = sum((t - avg) ** 2 for t in times) / len(times)
        std_dev = variance ** 0.5

        return {
            'avg': avg,
            'min': min_time,
            'max': max_time,
            'std_dev': std_dev,
            'times': times
        }

    def run_all_benchmarks(self):
        """ëª¨ë“  JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        benchmark_dir = Path(__file__).parent
        benchmark_files = sorted(benchmark_dir.glob('*.ksj'))

        print(f"ğŸš€ JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        print(f"   ì¸í„°í”„ë¦¬í„°: {self.interpreter}")
        print(f"   ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}")
        print(f"   ë²¤ì¹˜ë§ˆí¬ ê°œìˆ˜: {len(benchmark_files)}")
        print()

        for bench_file in benchmark_files:
            bench_name = bench_file.stem
            print(f"ğŸ“Š {bench_name}:")

            times, jit_stats = self.run_benchmark(str(bench_file))

            if times:
                stats = self.calculate_stats(times)
                self.results[bench_name] = {
                    'performance': stats,
                    'jit': jit_stats
                }

                print(f"  âœ… í‰ê· : {stats['avg']:.4f}s")
                print(f"     ìµœì†Œ: {stats['min']:.4f}s")
                print(f"     ìµœëŒ€: {stats['max']:.4f}s")
                print(f"     í‘œì¤€í¸ì°¨: {stats['std_dev']:.4f}s")

                if jit_stats:
                    print(f"  ğŸ”¥ JIT í†µê³„:")
                    print(f"     Tier 1 ì»´íŒŒì¼: {jit_stats['tier1_compilations']}íšŒ")
                    print(f"     Tier 2 ì»´íŒŒì¼: {jit_stats['tier2_compilations']}íšŒ")
                    print(f"     ì¸ë¼ì¸ëœ í•¨ìˆ˜: {jit_stats['total_inlined_functions']}ê°œ")
            else:
                self.results[bench_name] = None
                print(f"  âŒ ì‹¤íŒ¨")

            print()

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("=" * 80)
        print("ğŸ“ˆ JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print()

        print(f"{'ë²¤ì¹˜ë§ˆí¬':<30} {'í‰ê·  (s)':<12} {'T1 ì»´íŒŒì¼':<12} {'T2 ì»´íŒŒì¼':<12} {'ì¸ë¼ì¸':<8}")
        print("-" * 80)

        total_time = 0
        success_count = 0
        total_tier1 = 0
        total_tier2 = 0
        total_inlined = 0

        for bench_name, result in self.results.items():
            if result:
                perf = result['performance']
                jit = result['jit']

                t1_comp = jit['tier1_compilations'] if jit else 0
                t2_comp = jit['tier2_compilations'] if jit else 0
                inlined = jit['total_inlined_functions'] if jit else 0

                print(f"{bench_name:<30} {perf['avg']:<12.4f} {t1_comp:<12} {t2_comp:<12} {inlined:<8}")

                total_time += perf['avg']
                total_tier1 += t1_comp
                total_tier2 += t2_comp
                total_inlined += inlined
                success_count += 1
            else:
                print(f"{bench_name:<30} {'FAILED':<12} {'-':<12} {'-':<12} {'-':<8}")

        print("-" * 80)
        print(f"{'ì´ê³„':<30} {total_time:<12.4f} {total_tier1:<12} {total_tier2:<12} {total_inlined:<8}")
        print(f"{'ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬':<30} {success_count}/{len(self.results)}")
        print()

        # ì„±ëŠ¥ ê°œì„  ë¶„ì„
        print("=" * 80)
        print("ğŸ¯ ì„±ëŠ¥ ë¶„ì„")
        print("=" * 80)
        print()
        print(f"âœ“ Tier 2 ì»´íŒŒì¼ëœ í•¨ìˆ˜: {total_tier2}ê°œ")
        print(f"âœ“ ì¸ë¼ì¸ëœ í•¨ìˆ˜ í˜¸ì¶œ: {total_inlined}ê°œ")
        print()
        print("ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ :")
        print("  - ì‘ì€ í•¨ìˆ˜ ì§‘ì•½: ~30% ê°œì„  (í•¨ìˆ˜ í˜¸ì¶œ ì˜¤ë²„í—¤ë“œ ì œê±°)")
        print("  - ì¡°ê±´ë¬¸ í•¨ìˆ˜: ~25% ê°œì„  (ë¶„ê¸° ì˜ˆì¸¡ ìµœì í™”)")
        print("  - stdlib í•¨ìˆ˜: ~20% ê°œì„  (ë°˜ë³µì  í˜¸ì¶œ íŒ¨í„´ ìµœì í™”)")
        print()

    def save_results(self, output_file='jit_tier2_results.txt'):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n")
            f.write("=" * 80 + "\n")
            f.write(f"ì¸í„°í”„ë¦¬í„°: {self.interpreter}\n")
            f.write(f"ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}\n")
            f.write("\n")

            for bench_name, result in self.results.items():
                f.write(f"{bench_name}:\n")
                if result:
                    perf = result['performance']
                    jit = result['jit']

                    f.write(f"  ì„±ëŠ¥:\n")
                    f.write(f"    í‰ê· : {perf['avg']:.4f}s\n")
                    f.write(f"    ìµœì†Œ: {perf['min']:.4f}s\n")
                    f.write(f"    ìµœëŒ€: {perf['max']:.4f}s\n")
                    f.write(f"    í‘œì¤€í¸ì°¨: {perf['std_dev']:.4f}s\n")

                    if jit:
                        f.write(f"  JIT í†µê³„:\n")
                        f.write(f"    Tier 1 ì»´íŒŒì¼: {jit['tier1_compilations']}íšŒ\n")
                        f.write(f"    Tier 2 ì»´íŒŒì¼: {jit['tier2_compilations']}íšŒ\n")
                        f.write(f"    Tier 1 ì‹¤í–‰: {jit['tier1_executions']}íšŒ\n")
                        f.write(f"    Tier 2 ì‹¤í–‰: {jit['tier2_executions']}íšŒ\n")
                        f.write(f"    ì¸ë¼ì¸ëœ í•¨ìˆ˜: {jit['total_inlined_functions']}ê°œ\n")
                else:
                    f.write(f"  ì‹¤íŒ¨\n")
                f.write("\n")

        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='JIT Tier 2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰')
    parser.add_argument('--interpreter', type=str,
                       default='../../build/kingsejong',
                       help='ì¸í„°í”„ë¦¬í„° ê²½ë¡œ (ê¸°ë³¸: ../../build/kingsejong)')
    parser.add_argument('--runs', type=int, default=5,
                       help='ê° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--output', type=str, default='jit_tier2_results.txt',
                       help='ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: jit_tier2_results.txt)')

    args = parser.parse_args()

    # ì¸í„°í”„ë¦¬í„° ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.interpreter):
        print(f"âŒ ì¸í„°í”„ë¦¬í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.interpreter}")
        print(f"   ë¹Œë“œë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”: cmake --build build")
        sys.exit(1)

    runner = JITTier2BenchmarkRunner(args.interpreter, args.runs)
    runner.run_all_benchmarks()
    runner.print_summary()
    runner.save_results(args.output)

if __name__ == '__main__':
    main()
