#!/usr/bin/env python3
"""
KingSejong ì–¸ì–´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python3 run_benchmarks.py [--interpreter PATH] [--runs N]
"""

import subprocess
import time
import sys
import os
import argparse
from pathlib import Path

class BenchmarkRunner:
    def __init__(self, interpreter_path, num_runs=5):
        self.interpreter = interpreter_path
        self.num_runs = num_runs
        self.results = {}

    def run_benchmark(self, benchmark_file):
        """ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        times = []

        for i in range(self.num_runs):
            start = time.time()
            try:
                result = subprocess.run(
                    [self.interpreter, benchmark_file],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                end = time.time()

                if result.returncode != 0:
                    print(f"  âŒ ì‹¤í–‰ ì‹¤íŒ¨: {benchmark_file}")
                    print(f"     ì˜¤ë¥˜: {result.stderr}")
                    return None

                elapsed = end - start
                times.append(elapsed)
                print(f"  Run {i+1}/{self.num_runs}: {elapsed:.4f}s")

            except subprocess.TimeoutExpired:
                print(f"  â±ï¸ íƒ€ì„ì•„ì›ƒ: {benchmark_file}")
                return None
            except Exception as e:
                print(f"  âŒ ì˜ˆì™¸ ë°œìƒ: {e}")
                return None

        return times

    def calculate_stats(self, times):
        """í†µê³„ ê³„ì‚°"""
        if not times:
            return None

        avg = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)

        # í‘œì¤€í¸ì°¨ ê³„ì‚°
        variance = sum((t - avg) ** 2 for t in times) / len(times)
        std_dev = variance ** 0.5

        return {
            'avg': avg,
            'min': min_time,
            'max': max_time,
            'std_dev': std_dev,
            'times': times
        }

    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        benchmark_dir = Path(__file__).parent
        benchmark_files = sorted(benchmark_dir.glob('*.ksj'))

        print(f"ğŸš€ KingSejong ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        print(f"   ì¸í„°í”„ë¦¬í„°: {self.interpreter}")
        print(f"   ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}")
        print(f"   ë²¤ì¹˜ë§ˆí¬ ê°œìˆ˜: {len(benchmark_files)}")
        print()

        for bench_file in benchmark_files:
            bench_name = bench_file.stem
            print(f"ğŸ“Š {bench_name}:")

            times = self.run_benchmark(str(bench_file))

            if times:
                stats = self.calculate_stats(times)
                self.results[bench_name] = stats

                print(f"  âœ… í‰ê· : {stats['avg']:.4f}s")
                print(f"     ìµœì†Œ: {stats['min']:.4f}s")
                print(f"     ìµœëŒ€: {stats['max']:.4f}s")
                print(f"     í‘œì¤€í¸ì°¨: {stats['std_dev']:.4f}s")
            else:
                self.results[bench_name] = None
                print(f"  âŒ ì‹¤íŒ¨")

            print()

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print()

        print(f"{'ë²¤ì¹˜ë§ˆí¬':<20} {'í‰ê·  (s)':<12} {'ìµœì†Œ (s)':<12} {'ìµœëŒ€ (s)':<12}")
        print("-" * 60)

        total_time = 0
        success_count = 0

        for bench_name, stats in self.results.items():
            if stats:
                print(f"{bench_name:<20} {stats['avg']:<12.4f} {stats['min']:<12.4f} {stats['max']:<12.4f}")
                total_time += stats['avg']
                success_count += 1
            else:
                print(f"{bench_name:<20} {'FAILED':<12} {'-':<12} {'-':<12}")

        print("-" * 60)
        print(f"{'ì´ ì‹¤í–‰ ì‹œê°„':<20} {total_time:<12.4f}")
        print(f"{'ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬':<20} {success_count}/{len(self.results)}")
        print()

    def save_results(self, output_file='benchmark_results.txt'):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("KingSejong ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼\n")
            f.write("=" * 60 + "\n")
            f.write(f"ì¸í„°í”„ë¦¬í„°: {self.interpreter}\n")
            f.write(f"ì‹¤í–‰ íšŸìˆ˜: {self.num_runs}\n")
            f.write("\n")

            for bench_name, stats in self.results.items():
                f.write(f"{bench_name}:\n")
                if stats:
                    f.write(f"  í‰ê· : {stats['avg']:.4f}s\n")
                    f.write(f"  ìµœì†Œ: {stats['min']:.4f}s\n")
                    f.write(f"  ìµœëŒ€: {stats['max']:.4f}s\n")
                    f.write(f"  í‘œì¤€í¸ì°¨: {stats['std_dev']:.4f}s\n")
                    f.write(f"  ì‹¤í–‰ ì‹œê°„: {stats['times']}\n")
                else:
                    f.write(f"  ì‹¤íŒ¨\n")
                f.write("\n")

        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='KingSejong ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰')
    parser.add_argument('--interpreter', type=str,
                       default='../build/kingsejong',
                       help='ì¸í„°í”„ë¦¬í„° ê²½ë¡œ (ê¸°ë³¸: ../build/kingsejong)')
    parser.add_argument('--runs', type=int, default=5,
                       help='ê° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--output', type=str, default='benchmark_results.txt',
                       help='ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: benchmark_results.txt)')

    args = parser.parse_args()

    # ì¸í„°í”„ë¦¬í„° ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.interpreter):
        print(f"âŒ ì¸í„°í”„ë¦¬í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.interpreter}")
        print(f"   ë¹Œë“œë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”: cmake --build build")
        sys.exit(1)

    runner = BenchmarkRunner(args.interpreter, args.runs)
    runner.run_all_benchmarks()
    runner.print_summary()
    runner.save_results(args.output)

if __name__ == '__main__':
    main()
